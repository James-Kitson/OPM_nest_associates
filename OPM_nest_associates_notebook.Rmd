---
title: "R Notebook"
output: html_notebook
---

#OPM Sanger processing

###This [R Markdown](http://rmarkdown.rstudio.com) notebook has been written to outline the steps taken to quality trim and merge sanger sequences for Oak Processionary Moth nest associates. The code used here draws heavily on the [sangeranalyseR](https://github.com/roblanf/sangeranalyseR) vignette provided by [Robert Lanfear](http:www.robertlanfear.com).

##Part one - Set up the libraries we need.
We're going to be using a library called [sangeranalyseR](https://github.com/roblanf/sangeranalyseR) so that the process is more documented and reproducible compared to the usual trimming and merging by eye performed using Geneious, FinchTV or similar programs. There are quite a few required libraries here but the script called [installs_for_sangerseqR.R](https://github.com/James-Kitson/OPM_nest_associates/blob/master/installs_for_sangerseqR.R) contains all the relevant commands.

```{r}
#Clear the workspace
rm(list=ls())
#Load the library we need
library(sangeranalyseR)
```

##Part two - check trimming parameters
It's always good to check your trimmed output to make sure that you understand what the trimming and merging process we'll use later has done. It is important that the trimming parameters set below are identical to the ```make.consensus.seqs``` parameters used later so that you are seeing the same data.

```{r}
#Define the folder containing the .ab1 sequences
input.folder<-"ab1_sequences/"

#Make a file list to process
input.files<-list.files(input.folder)

#Use a for loop to trim, check for secondary peaks and write a pdf chromatogram for each sequence
for(i in 1:length(input.files)){
  seq.abif<-read.abif(paste(input.folder,input.files[i], sep=""))
  seq.sanger<-sangerseq(seq.abif)
  sp<-secondary.peaks(seq.sanger, output.folder="trimmed_seq_figures", file.prefix = paste(input.files[i]))
}
```


##Part three - Make consensus sequences of paired reads.
We will be automating the trimming and merging processes as there's quite a few to deal with. For this, we're going to be using the ```make.consensus.seqs``` approach.

```{r}
#Define the suffixes that denote the sequence direction
forward.suffix<-"_HCO2198puc.ab1"
reverse.suffix<-"_LCO1490puc.ab1"

#Make the readsets using the suffixes and only keep trimmed sequences longer than 250bp
consensus.seqs<-make.consensus.seqs(input.folder, forward.suffix, reverse.suffix, trim=TRUE, min.length=250)

#This outputs the read summaries
consensus.seqs$read.summaries

#This outputs the consensus sequence summaries
consensus.seqs$consensus.summaries

#This outputs the actual sequences
consensus.seqs$consensus.sequences

#This outputs the aligned sequences
BrowseSeqs(consensus.seqs$consensus.alignment)
```

##Part four - output the processed sequences for analysis

```{r}
#write the merged sequences to FASTA format
write.dna(consensus.seqs$consensus.sequences, file="Processed_reads.fasta", format='fasta', nbcol=-1, colw = 1000000)
```


